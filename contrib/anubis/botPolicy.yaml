## Anubis has the ability to let you import snippets of configuration into the main
## configuration file. This allows you to break up your config into smaller parts
## that get logically assembled into one big file.
##
## Of note, a bot rule can either have inline bot configuration or import a
## bot config snippet. You cannot do both in a single bot rule.
##
## Import paths can either be prefixed with (data) to import from the common/shared
## rules in the data folder in the Anubis source tree or will point to absolute/relative
## paths in your filesystem. If you don't have access to the Anubis source tree, check
## /usr/share/docs/anubis/data or in the tarball you extracted Anubis from.
logging:
  sink: file
  parameters:
    file: "/data/anubis.log"
    maxBackups: 5
    maxBytes: 104857600  # 100 MiB in bytes
    maxAge: 30 # days

bots:
  # You can import the entire default config with this macro:
  # - import: (data)/meta/default-config.yaml

  # Pathological bots to deny
  - # This correlates to data/bots/_deny-pathological.yaml in the source tree
    # https://github.com/TecharoHQ/anubis/blob/main/data/bots/_deny-pathological.yaml
    import: (data)/bots/_deny-pathological.yaml
  - import: (data)/bots/aggressive-brazilian-scrapers.yaml

  # Aggressively block AI/LLM related bots/agents by default
  - import: (data)/meta/ai-block-aggressive.yaml

  # Consider replacing the aggressive AI policy with more selective policies:
  # - import: (data)/meta/ai-block-moderate.yaml
  # - import: (data)/meta/ai-block-permissive.yaml

  # Search engine crawlers to allow, defaults to:
  #   - Google (so they don't try to bypass Anubis)
  #   - Apple
  #   - Bing
  #   - DuckDuckGo
  #   - Qwant
  #   - The Internet Archive
  #   - Kagi
  #   - Marginalia
  #   - Mojeek
  - import: (data)/crawlers/_allow-good.yaml
  # Challenge Firefox AI previews
  - import: (data)/clients/x-firefox-ai.yaml

  # Allow common "keeping the internet working" routes (well-known, favicon, robots.txt)
  - import: (data)/common/keep-internet-working.yaml

  # # Punish any bot with "bot" in the user-agent string
  # # This is known to have a high false-positive rate, use at your own risk
  # - name: generic-bot-catchall
  #   user_agent_regex: (?i:bot|crawler)
  #   action: CHALLENGE
  #   challenge:
  #     difficulty: 16  # impossible
  #     report_as: 4    # lie to the operator
  #     algorithm: slow # intentionally waste CPU cycles and time

  # Requires a subscription to Thoth to use, see
  # https://anubis.techaro.lol/docs/admin/thoth#geoip-based-filtering
  #- name: countries-with-aggressive-scrapers
  #  action: WEIGH
  #  geoip:
  #    countries:
  #      - BR
  #      - CN
  #  weight:
  #    adjust: 10

  # Requires a subscription to Thoth to use, see
  # https://anubis.techaro.lol/docs/admin/thoth#asn-based-filtering
  #- name: aggressive-asns-without-functional-abuse-contact
  #  action: WEIGH
  #  asns:
  #    match:
  #      - 13335 # Cloudflare
  #      - 136907 # Huawei Cloud
  #      - 45102 # Alibaba Cloud
  #  weight:
  #    adjust: 10

  ## System load based checks.
  # If the system is under high load, add weight.
  - name: high-load-average
    action: WEIGH
    expression: load_1m >= 8.0 # make sure to end the load comparison in a .0
    weight:
      adjust: 20

  # If your backend service is running on the same operating system as Anubis,
  # you can uncomment this rule to make the challenge easier when the system is
  # under low load.
  #
  # If it is not, remove weight.
  - name: low-load-average
    action: WEIGH
    expression: load_15m <= 4.0 # make sure to end the load comparison in a .0
    weight:
      adjust: -10

  # Assert behaviour that only genuine browsers display. This ensures that Chrome
  # or Firefox versions
  - name: realistic-browser-catchall
    expression:
      all:
        - '"User-Agent" in headers'
        - '( userAgent.contains("Firefox") ) || ( userAgent.contains("Chrome") ) || ( userAgent.contains("Safari") )'
        - '"Accept" in headers'
        - '"Sec-Fetch-Dest" in headers'
        - '"Sec-Fetch-Mode" in headers'
        - '"Sec-Fetch-Site" in headers'
        - '"Accept-Encoding" in headers'
        - '( headers["Accept-Encoding"].contains("zstd") || headers["Accept-Encoding"].contains("br") )'
        - '"Accept-Language" in headers'
    action: WEIGH
    weight:
      adjust: -10

  # The Upgrade-Insecure-Requests header is typically sent by browsers, but not always
  - name: upgrade-insecure-requests
    expression: '"Upgrade-Insecure-Requests" in headers'
    action: WEIGH
    weight:
      adjust: -2

  # Chrome should behave like Chrome
  - name: chrome-is-proper
    expression:
      all:
        - userAgent.contains("Chrome")
        - '"Sec-Ch-Ua" in headers'
        - 'headers["Sec-Ch-Ua"].contains("Chromium")'
        - '"Sec-Ch-Ua-Mobile" in headers'
        - '"Sec-Ch-Ua-Platform" in headers'
    action: WEIGH
    weight:
      adjust: -5

  - name: should-have-accept
    expression: '!("Accept" in headers)'
    action: WEIGH
    weight:
      adjust: 5

  # Generic catchall rule
  - name: generic-browser
    user_agent_regex: >-
      Mozilla|Opera
    action: WEIGH
    weight:
      adjust: 10

  # For testing
  - name: anubis-underworld
    path_regex: ^/ANUBIS-UNDERWORLD$
    action: WEIGH
    weight:
      adjust: 30

  - name: anubis-deny
    path_regex: ^/ANUBIS-DENY$
    action: DENY

  # Actually allow all kinds of user agents that do NOT claim to be browsers
  # because all the bad AI crawlers pretend to be browsers
  - name: whoso-walketh-uprightly-shall-pass-unhindered
    user_agent_regex: >-
      Mojo|Perl|curl|Wget|requests|urllib|libwww|aria2|open[qQ][aA]|osc
    action: ALLOW

  # For obvious reasons
  - name: api-routes
    path_regex: ^/api/.*$
    action: ALLOW

  # AI bots want text (hopefully)
  - name: needles-routes
    path_regex: ^/needles/.*$
    action: ALLOW

  # AI bots want text (hopefully) - also /image is directly served by nginx
  - name: images-routes
    path_regex: ^/images?/.*$
    action: ALLOW

  # Served by nginx
  - name: assets-route
    path_regex: ^/assets.*$
    action: ALLOW

  # might be used by our bots and nobody knows this URL anyways
  - name: sc-route
    path_regex: ^/snapshot-changes.*$
    action: ALLOW

  - name: nginx-status-route
    path_regex: ^/nginx_status.*$
    action: ALLOW

  - name: munin-route
    path_regex: ^/munin.*$
    action: ALLOW

  # Requires login anyways
  - name: admin-routes
    path_regex: ^/admin/.*$
    action: ALLOW

  # Won't work otherwise
  - name: health-route
    path_regex: ^/health.*$
    action: ALLOW

  # This has some irony
  - name: allow-our-own-ai
    path_regex: ^/experimental/mcp.*$
    action: ALLOW

  # For now AI bots are not good at JS, so allow AJAX
  - name: allow-ajax
    path_regex: ^.*ajax.*$
    action: ALLOW

  - name: allow-ws
    path_regex: ^.*ws-console.*$
    action: ALLOW

  - name: allow-ws2
    path_regex: ^.*liveviewhandler.*$
    action: ALLOW


dnsbl: false

# #
# impressum:
#   # Displayed at the bottom of every page rendered by Anubis.
#   footer: >-
#     This website is hosted by Zombocom. If you have any complaints or notes
#     about the service, please contact
#     <a href="mailto:contact@domainhere.example">contact@domainhere.example</a>
#     and we will assist you as soon as possible.

#   # The imprint page that will be linked to at the footer of every Anubis page.
#   page:
#     # The HTML <title> of the page
#     title: Imprint and Privacy Policy
#     # The HTML contents of the page. The exact contents of this page can
#     # and will vary by locale. Please consult with a lawyer if you are not
#     # sure what to put here
#     body: >-
#       <p>Last updated: June 2025</p>

#       <h2>Information that is gathered from visitors</h2>

#       <p>In common with other websites, log files are stored on the web server saving details such as the visitor's IP address, browser type, referring page and time of visit.</p>

#       <p>Cookies may be used to remember visitor preferences when interacting with the website.</p>

#       <p>Where registration is required, the visitor's email and a username will be stored on the server.</p>

#       <!-- ... -->

# Open Graph passthrough configuration, see here for more information:
# https://anubis.techaro.lol/docs/admin/configuration/open-graph/
openGraph:
  # Enables Open Graph passthrough
  enabled: false
  # Enables the use of the HTTP host in the cache key, this enables
  # caching metadata for multiple http hosts at once.
  considerHost: false
  # How long cached OpenGraph metadata should last in memory
  ttl: 24h
  # # If set, return these opengraph values instead of looking them up with
  # # the target service.
  # #
  # # Correlates to properties in https://ogp.me/
  # override:
  #   # og:title is required, it is the title of the website
  #   "og:title": "Techaro Anubis"
  #   "og:description": >-
  #     Anubis is a Web AI Firewall Utility that helps you fight the bots
  #     away so that you can maintain uptime at work!
  #   "description": >-
  #     Anubis is a Web AI Firewall Utility that helps you fight the bots
  #     away so that you can maintain uptime at work!

# By default, send HTTP 200 back to clients that either get issued a challenge
# or a denial. This seems weird, but this is load-bearing due to the fact that
# the most aggressive scraper bots seem to really, really, want an HTTP 200 and
# will stop sending requests once they get it.
status_codes:
  CHALLENGE: 200
  DENY: 200

# Anubis can store temporary data in one of a few backends. See the storage
# backends section of the docs for more information:
#
# https://anubis.techaro.lol/docs/admin/policies#storage-backends
store:
  backend: bbolt
  parameters: {path: "/data/anubis.bdb"}

# The weight thresholds for when to trigger individual challenges. Any
# CHALLENGE will take precedence over this.
#
# A threshold has four configuration options:
#
#   - name: the name that is reported down the stack and used for metrics
#   - expression: A CEL expression with the request weight in the variable
#     weight
#   - action: the Anubis action to apply, similar to in a bot policy
#   - challenge: which challenge to send to the user, similar to in a bot policy
#
# See https://anubis.techaro.lol/docs/admin/configuration/thresholds for more
# information.
thresholds:
  # By default Anubis ships with the following thresholds:
  - name: minimal-suspicion # This client is likely fine, its soul is lighter than a feather
    expression: weight <= 0 # a feather weighs zero units
    action: ALLOW # Allow the traffic through
  # For clients that had some weight reduced through custom rules, give them a
  # lightweight challenge.
  - name: mild-suspicion
    expression:
      all:
        - weight > 0
        - weight < 10
    action: CHALLENGE
    challenge:
      # https://anubis.techaro.lol/docs/admin/configuration/challenges/metarefresh
      algorithm: metarefresh
      difficulty: 1
      report_as: 1
  # For clients that are browser-like but have either gained points from custom rules or
  # report as a standard browser.
  - name: moderate-suspicion
    expression:
      all:
        - weight >= 10
        - weight < 20
    action: CHALLENGE
    challenge:
      # https://anubis.techaro.lol/docs/admin/configuration/challenges/proof-of-work
      algorithm: fast
      difficulty: 2 # two leading zeros, very fast for most clients
      report_as: 2
  - name: mild-proof-of-work
    expression:
      all:
        - weight >= 20
        - weight < 30
    action: CHALLENGE
    challenge:
      # https://anubis.techaro.lol/docs/admin/configuration/challenges/proof-of-work
      algorithm: fast
      difficulty: 3
      report_as: 3
  # For clients that are browser like and have gained many points from custom rules
  - name: extreme-suspicion
    expression: weight >= 30
    action: CHALLENGE
    challenge:
      # https://anubis.techaro.lol/docs/admin/configuration/challenges/proof-of-work
      algorithm: fast
      difficulty: 5
      report_as: 5

