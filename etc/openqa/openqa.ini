[global]
## Web site name for tab titles and bookmarks
#appname = openQA

## type of branding - [ openSUSE, plain, openqa.suse.de ]
#branding = plain

## type and location of needle repo
#scm = git

## space separated list of domains from which asset download with
## _URL params is allowed. Matched at the end of the hostname in
## the URL. with these values downloads from opensuse.org,
## dl.fedoraproject.org, and a.b.c.opensuse.org are allowed; downloads
## from moo.net, dl.opensuse and fedoraproject.org.evil are not
## default is undefined, meaning asset download is *disabled*, you
## must set this option to enable it
#download_domains = fedoraproject.org opensuse.org

## set if you have a local repo mirror
#suse_mirror = http://FIXME

## base url [default: $self->req->url->base]
#base_url = http://FIXME

# days for Strict-Transport-Security, 0 to not add this header
# http://en.wikipedia.org/wiki/Strict-Transport-Security
# hsts = 365

## Set to 0 to disable auditing backend
# audit_enabled = 1

## Set to 1 to enable profiling
## * Needs Mojolicious::Plugin::NYTProf
## * Profiling data will be PUBLICLY accessible under /nytprof route.
## * The plugin impairs performance and the generated profiling data might quickly
##   utilize a lot of disk space. So don't enable this plugin in production.
# profiling_enabled = 0

## Set to 1 to enable monitoring
## * Needs Mojolicious::Plugin::Status
## * Monitoring will be accessible to operators and admins under /monitoring route.
## * The plugin can impair performance significantly with prefork workers enabled.
##   So don't enable this plugin in production.
# monitoring_enabled = 0

## space-separated list of extra plugins to load; plugin must be under
## OpenQA::WebAPI::Plugin and correctly-cased module name given here,
## this example loads OpenQA::WebAPI::Plugin::AMQP
#plugins = AMQP

## space-separated list of asset types *not* to show links for in the
# web UI. Default is 'repo'
#hide_asset_types = repo iso

## Recognized referers contains list of hostnames separated by space. When
# openQA detects (via 'Referer' header) that test job was accessed from
# this domain, this job is labeled as linked and considered as important
# recognized_referers = bugzilla.suse.com bugzilla.opensuse.org bugzilla.novell.com bugzilla.microfocus.com progress.opensuse.org github.com

## A regex in a string of test settings to ignore for "job investigation"
#job_investigate_ignore = "(JOBTOKEN|NAME)"

# Specify the number of seconds until an unresponsive worker is considered offline
# and its currently assigned jobs are taken care of by the stale job detection
#worker_timeout = 1800

## Timeout for the git command in "job investigation"
#job_investigate_git_timeout = 20

## Gracefully restart the prefork workers if they reach a certain memory limit (in kB)
#max_rss_limit = 180000

## Causes jobs reported as incomplete by the worker to be cloned automatically when the
## reason matches; set to 0 to disable
#auto_clone_regex = ^(cache failure: |terminated prematurely: |api failure: Failed to register .* 503|backend died: .*VNC.*Connection timed out))

## A regex pattern that a "force_result" label description in job comments
## must match to be accepted. If undefined and by default no rules are applied
## and no description is expected
#force_result_regex =

#[scm git]
# name of remote to get updates from before committing changes (e.g. origin, leave out-commented to disable remote update)
#update_remote = origin
# name of branch to rebase against before committing changes (e.g. origin/master, leave out-commented to disable rebase)
#update_branch = origin/master
# whether committed changes should be pushed
#do_push = no

## Authentication method to use for user management
[auth]
# method = Fake|OpenID|OAuth2

# for GitHub, salsa.debian.org and providers listed on https://metacpan.org/pod/Mojolicious::Plugin::OAuth2#Configuration
# one can use:
#[oauth2]
#provider = github|debian_salsa
#key = ...
#secret = ...

# alternatively, one can specify parameters manually without relying on magic a provider name:
#[oauth2]
#provider = custom
#unique_name = debian_salsa
#key = ...
#secret = ...
#authorize_url = https://salsa.debian.org/oauth/authorize?response_type=code
#token_url = https://salsa.debian.org/oauth/token
#user_url  = https://salsa.debian.org/api/v4/user
#token_scope = read_user
#token_label = Bearer
#nickname_from = username

[logging]
#logging is to stderr (so systemd journal) by default
#if you use a file, remember the apparmor profile!
#file = /var/log/openqa
#level = debug
#sql_debug = true

## Configuration for OpenID auth method
[openid]
## base url for openid provider
#provider = https://www.opensuse.org/openid/user/
## enforce redirect back to https
#httpsonly = 1

## Configuration for auditing plugin
[audit]
# disable auditing of chatty events by default
blocklist = job_grab job_done

# Sets the storage duration in days for the different audit event types
[audit/storage_duration]
# By default cleanup is disabled, see http://open.qa/docs/#_auditing_tracking_openqa_changes
# The following categories with example values can be uncommented as needed
#startup = 10
#jobgroup = 365
#jobtemplate = 365
#table = 365
#iso = 60
#user = 60
#asset = 30
#needle = 30
#other = 15

## Configuration for AMQP plugin
[amqp]
#heartbeat_timeout = 60
#reconnect_timeout = 5
# guest/guest is the default anonymous user/pass for RabbitMQ
#url = amqp://guest:guest@localhost:5672/
#exchange = pubsub
#topic_prefix = suse

# Cleanup related configuration (besides limits)
[cleanup]
# Specifies whether different cleanup tasks can run in parallel. This option
# makes particularly sense if assets and results are stored on different storage
# locations or if your common storage solution is performant enough.
concurrent = 0

# Default limits for cleanup (sizes are in GiB, durations in days, zero denotes infinity)
[default_group_limits]
#asset_size_limit = 100 # only used on job group level (parent groups have no default)
#log_storage_duration = 30
#important_log_storage_duration = 120
#result_storage_duration = 365
#important_result_storage_duration = 0

[minion_task_triggers]
# Specify one or more task names (space-separated), by default these are not enabled.
# Good candidates would be limit_assets or limit_results_and_logs.
# This is analoguous to triggering tasks via systemd timers using
# openqa-enqueue-asset-cleanup or openqa-enqueue-result-cleanup except
# it's triggered whenever a job is done rather than periodically.
#on_job_done = limit_results_and_logs limit_assets

[misc_limits]
#untracked_assets_storage_duration = 14
# Performs the cleanup of results/assets only if the free disk space on the relevant partition is below the specified percentage (and aborts early otherwise)
#result_cleanup_max_free_percentage = 100
#asset_cleanup_max_free_percentage = 100
# Specify the screenshot ID range to query at once from the database (reduce to avoid big queries, increase to lower query overhead)
#screenshot_cleanup_batch_size = 200000
# Specify the number of screenshot ID ranges (with a size as configured by screenshot_cleanup_batch_size) to process in a single Minion
# job (reduce to avoid Minion jobs from running very long and possibly being interrupted, increase to reduce the number of Minion jobs)
#screenshot_cleanup_batches_per_minion_job = 450
# Extends the job result cleanup to ensure the partition results are stored on does not become too full
# (still experimental, relies on df)
#results_min_free_disk_space_percentage = 0

[archiving]
# Moves logs of jobs which are preserved during the cleanup because they are considered important
# to "${OPENQA_ARCHIVEDIR:-${OPENQA_BASEDIR:-/var/lib}/openqa/archive}/testresults"
#archive_preserved_important_jobs = 0

[job_settings_ui]
# Specify the keys of job settings which reference a file and should therefore be rendered
# as links to those files within the job settings tab.
# Directories should be under the `CASEDIR` root path or under the `data` folder of the `CASEDIR`. The `data`
# folder is used as default but it can be configured to cover needs of any test distribution. To change it, add the
# `default_data_dir` variable with the name of the directory.
#keys_to_render_as_links=YAML_SCHEDULE,AUTOYAST

[hooks]
# Specify custom hook scripts format `job_done_hook_$result` to be called when
# a job is done. Any executable specified in the variable as absolute path or
# executable name in `$PATH` is called with the job ID as first and only
# parameter corresponding to the `$result`, for example
#job_done_hook_failed = my-openqa-hook-failed

# Configuration for InfluxDB routes
[influxdb]
# Specify Minion task names which should never be counted towards the total of number failed Minion jobs.
#ignored_failed_minion_jobs = influxdb-minion-fail-job-task-name
